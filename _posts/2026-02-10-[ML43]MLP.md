---
layout: post
title: "Multilayer Perceptrons"
date: 2026-02-10 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, MLP]
tags: [Artificial Intelligence, MLP]
pin: false
math: true
mermaid: true
---

# From Linear Classifiers to Multilayer Perceptrons ðŸš€

> Complete lecture-style notes with full derivations and explanations.

---

## 1. Issues with Linear Classifiers

$$
f(x) = Wx
$$

Linear classifiers:
- Learn one template per class
- Can only form linear decision boundaries
- Fail on non-linearly separable data

---

## 2. Feature Motivation

$$
x \rightarrow z \rightarrow y
$$

Mapping to feature space can enable linear separation.

---

## 3. Image Features

- Color histogram
- HOG
- Bag-of-Words

---

## 4. Perceptron

$$
y = f(w_1 x_1 + w_2 x_2)
$$

$$
f(a) =
\begin{cases}
1 & a > \theta \\
0 & a \le \theta
\end{cases}
$$

---

## 5. Multilayer Perceptron

$$
h = \sigma(W_1 x), \quad y = W_2 h
$$

---

## 6. Activation Functions

$$
\sigma(x) = \frac{1}{1 + e^{-x}}, \quad \text{ReLU}(x) = \max(0, x)
$$

---

## 7. Backpropagation

Loss:
$$
\mathcal{L} = (\hat{y} - y)^2
$$

Gradients:
$$
\frac{\partial \mathcal{L}}{\partial W_2} = 2(\hat{y}-y)h
$$

$$
\frac{\partial \mathcal{L}}{\partial W_1} = 2(\hat{y}-y)W_2 h(1-h)x
$$

---

## 8. Summary

Linear â†’ Features â†’ Perceptrons â†’ MLP â†’ Backprop
