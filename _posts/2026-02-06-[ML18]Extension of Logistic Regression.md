---
layout: post
title: "Extension of Logistic Regression"
date: 2026-02-06 00:00:00 +0900
author: kang
categories: [Machince Learning, Classification]
tags: [Machince Learning, Mathematics, ML, Supervised, Regression, Classification]
pin: false
math: true
mermaid: true
---

# ğŸ“˜ Revised Supervised Learning & Logistic Regression

> ğŸ¯ Complete version â€” **no omission**, full equations, clean blog styling with emojis  
> ğŸ§  Covers: supervised learning framework â†’ logistic regression â†’ softmax â†’ MLE â†’ logâ€‘loss  

---

# ğŸ¤– Revised Supervised Learning Framework

## Overview

Machine learning is a **dataâ€‘driven approach**.  
Instead of manually designing rules, we design a **model form**, and data determines the optimal parameters.

---

## ğŸ§­ Stepâ€‘byâ€‘Step Framework

### 1ï¸âƒ£ Model Design

Humans specify the **structure of the model**.

Example â€” Logistic Regression:

$$
P(y = 1 \mid \mathbf{x}) =
\frac{1}{1 + e^{-\boldsymbol{\beta}^T \mathbf{x}}}
$$

---

### 2ï¸âƒ£ Define the Learning Goal

We want the model to approximate:

$$
y \approx f(\mathbf{x})
$$

as accurately as possible.

---

### 3ï¸âƒ£ Learn Parameters from Data

Given training data:

$$
\{(\mathbf{x}_i, y_i)\}_{i=1}^{n}
$$

we estimate optimal parameters $\boldsymbol{\beta}$.

---

#### ğŸ”¹ Step 3â€‘1. Prediction

Compute prediction:

$$
\hat{y} = f(\mathbf{x})
$$

---

#### ğŸ”¹ Step 3â€‘2. Loss Evaluation

Measure prediction quality using a **loss function**.

For logistic regression:

$$
\ell(y,\hat{y})
=
- y \log \hat{y}
- (1-y)\log(1-\hat{y})
$$

This is **logâ€‘loss / crossâ€‘entropy**.

---

#### ğŸ”¹ Step 3â€‘3. Parameter Update

Update parameters to reduce loss:

$$
\boldsymbol{\beta} \leftarrow \boldsymbol{\beta}
- \eta \nabla_{\boldsymbol{\beta}} \ell
$$

---

#### ğŸ”¹ Step 3â€‘4. Iteration

Repeat:

Prediction â†’ Loss â†’ Update â†’ Convergence

Goal:

$$
\hat{y} \approx y
$$

for most samples.

---

### 4ï¸âƒ£ Inference

For new input $\mathbf{x}$:

$$
\hat{y} = \arg\max_y P(y \mid \mathbf{x})
$$

---

## ğŸ’¡ Key Insight

Training = **minimizing loss = learning parameters from data**.

---

# ğŸ“ˆ Logistic Regression â€” Model & Extensions

## Binary Logistic Model

$$
P(y=1 \mid \mathbf{x})
=
\frac{1}{1+e^{-\boldsymbol{\beta}^T\mathbf{x}}}
$$

For single feature:

$$
P(y=1 \mid x)
=
\frac{1}{1+e^{-(\beta_0+\beta_1x)}}
$$

---

## Multiâ€‘Feature Extension

For $p$ features:

$$
\log\frac{p}{1-p}
=
\beta_0+\beta_1x_1+\cdots+\beta_px_p
$$

Parameters:

- $\beta_0$ : intercept  
- $\beta_1 \ldots \beta_p$ : feature weights  

Vector form:

$$
P(y=1 \mid \mathbf{x})
=
\frac{1}{1+e^{-\boldsymbol{\beta}^T\mathbf{x}}}
$$

---

## Multiâ€‘Class Extension â€” Softmax

Define class score:

$$
s_k = \boldsymbol{\beta}_k^T \mathbf{x}
$$

Probability:

$$
P(y=k \mid \mathbf{x})
=
\frac{e^{s_k}}{\sum_{j=1}^{K}e^{s_j}}
$$

Properties:

- $0 \le P \le 1$
- $\sum_k P = 1$

Sigmoid = special case of softmax with $K=2$.

---

# ğŸ“ Logistic Regression â€” Maximum Likelihood

## Conditional Logâ€‘Likelihood

$$
\hat{\boldsymbol{\beta}}
=
\arg\max_{\boldsymbol{\beta}}
\sum_{i=1}^{n}\log P(y_i \mid \mathbf{x}_i)
$$

---

## Expand Using Logistic Model

$$
p(y=1 \mid \mathbf{x})
=
\frac{1}{1+e^{-\boldsymbol{\beta}^T\mathbf{x}}}
$$

$$
p(y=0 \mid \mathbf{x})
=
\frac{1}{1+e^{\boldsymbol{\beta}^T\mathbf{x}}}
$$

---

## Logâ€‘Likelihood Form

$$
\sum_{i=1}^{N}
\left[
y_i \log \frac{1}{1+e^{-\boldsymbol{\beta}^T\mathbf{x}_i}}
+
(1-y_i)\log\frac{1}{1+e^{\boldsymbol{\beta}^T\mathbf{x}_i}}
\right]
$$

---

## Equivalent Minimization (Negative Logâ€‘Likelihood)

$$
\hat{\boldsymbol{\beta}}
=
\arg\min_{\boldsymbol{\beta}}
\sum_{i=1}^{N}
\left[
- y_i \log(1+e^{-\boldsymbol{\beta}^T\mathbf{x}_i})
-
(1-y_i)\log(1+e^{\boldsymbol{\beta}^T\mathbf{x}_i})
\right]
$$

---

## Logâ€‘Loss / Binary Crossâ€‘Entropy

$$
\ell(\boldsymbol{\beta})
=
\sum_{i=1}^{N}
\left[
- y_i \log(1+e^{-\boldsymbol{\beta}^T\mathbf{x}_i})
-
(1-y_i)\log(1+e^{\boldsymbol{\beta}^T\mathbf{x}_i})
\right]
$$

---

## ğŸ§  Final Insights

- Logistic regression **maximizes conditional likelihood**
- Equivalent to **minimizing logâ€‘loss**
- Convex â†’ unique global optimum
- Optimized using:
  - Gradient Descent  
  - SGD / Miniâ€‘batch  
  - Newton / Quasiâ€‘Newton  
