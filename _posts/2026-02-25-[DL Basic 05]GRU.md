---
layout: post
title: "05. Gated Recurrent Units (GRU)"
date: 2026-02-10 00:00:00 +0900
author: kang
categories: [Deep Learning, Foundations]
tags: [Artificial Intelligence, RNN, LSTM, GRU]
pin: false
math: true
mermaid: true
---

---
# <b>Gated Recurrent Units (GRU)</b>
---
### <b>Prerequisites</b>
    1. Recurrent Neural Network
    2. LSTM
<b>Recurrent Neural Network</b> have a problem when training owing to update gradient without insecure like <span style="color:#FFD5D5">vanishing or exploding gradient</span>. This leads to exponential growth or decay of gradients.
LSTM is good solution of gradient issue. but it is not also completely solving the problem.
    

---
## <b>What is Gated Recurrent Units (GRU)</b>

### 1. What is Gated Recurrent Units (GRU)?
- A <b>Model that is similar structure with RNN parameters</b></span>. But it is more preventing gradient update issue while long term sequence.
    
- <b>Structure</b>

    $$
    Input \rightarrow  GRU \rightarrow  GRU \rightarrow  GRU ... \rightarrow  Softmax \rightarrow  Output
    $$

- Can have more secure model with convex combination 
- it is also having vanishing or exploding gradient problem if sequence is too long. 


### 2. Why use Gated Recurrent Units (GRU)?
     1. Keep Memory
     2. convex combination

Simpler alternative to LSTM.
No separate cell state.

### 3. How use Recurrent Neural Network(RNN)?
    Hidden State

![GRU-inner](/assets/img/develop/GRU-inner.png)

- $ Reset Gate: r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r) $
- $ Update Gate: z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z) $
- $ Candidate Hidden: \tilde{h}_t = \tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h)$
- $ Final Hidden: h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t $


![RNN-process](/assets/img/develop/RNN-process.png)


### 4. What is <span style="color:red">FEW</span> PROBLEM of Recurrent Neural Network(RNN)?
    1. Vanishing Gradient
    2. Exploding Gradient

It is also remain the gradient problem. but it is more better than Vanilla RNN