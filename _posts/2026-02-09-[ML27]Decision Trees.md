---
layout: post
title: "Decision Trees"
date: 2026-02-09 00:00:00 +0900
author: kang
categories: [Machince Learning, Model]
tags: [Machince Learning, Mathematics, Model, Decision Trees]
pin: false
math: true
mermaid: true
---

# ğŸŒ³ Decision Trees

Decision trees are intuitive, interpretable models that **split the feature space into simple regions**.

---

# 1ï¸âƒ£ Tree Terminology Review

## ğŸŒ² General Tree Structure

A general tree **T** is partitioned into:

- **Root node** r  
- A set of **subtrees** attached to the root  

Example structure:

        A (root)
       / \
      B   C
     /|\
    D E F

---

## ğŸ“˜ Basic Terminology

| Term | Meaning |
|------|---------|
| ğŸ”µ Node (Vertex) | A point in the tree |
| â– Edge | Connection between nodes |
| ğŸ‘¨ Parent | Node with children |
| ğŸ‘¶ Child | Node below parent |
| ğŸ‘¥ Siblings | Nodes with same parent |
| ğŸŒ³ Root | Top node |
| ğŸƒ Leaf | Node with no children |
| â¬† Ancestor | Any node above |
| â¬‡ Descendant | Any node below |
| ğŸŒ¿ Subtree | Tree inside tree |

---

# 2ï¸âƒ£ Tree-Based Learning

## ğŸ¯ Core Idea

Tree-based learning **segments the predictor space into simple regions**.

âœ” Recursive partitioning  
âœ” Forms decision rules  
âœ” Works for both:

- ğŸ“ˆ Regression  
- ğŸ” Classification  

---

## âš™ï¸ How It Works

1. Choose best feature & split value  
2. Divide feature space  
3. Repeat recursively  
4. Leaf â†’ prediction  

---

## ğŸ“Š Interpretation

- Space split into **rectangular regions**
- Prediction is **constant inside each region**
- Model behaves like a **step function**

---

# 3ï¸âƒ£ Introduction to Decision Trees

## âœ… Pros

- ğŸ§  Easy to interpret
- ğŸ” Transparent decision rules
- âš–ï¸ No feature scaling required
- ğŸ”„ Handles nonlinear relationships
- ğŸ“Š Works with mixed data types

---

## âŒ Cons

- ğŸ“‰ Often lower prediction accuracy
- âš ï¸ High variance (unstable)
- ğŸŒª Prone to overfitting

---

# 4ï¸âƒ£ Ensemble Methods ğŸŒ²ğŸŒ²ğŸŒ²

Multiple trees â†’ Stronger model

| Method | Idea |
|--------|------|
| ğŸ² Bagging | Reduce variance |
| ğŸŒ² Random Forest | Randomized bagging |
| ğŸš€ Boosting | Reduce bias sequentially |

â¡ Dramatically improves accuracy

---

# 5ï¸âƒ£ Model Flexibility vs Interpretability

| Model | Flexibility | Interpretability |
|-------|------------|------------------|
| Linear Models | Low | High |
| Decision Trees | Medium | Medium |
| Random Forest / Boosting | High | Low |
| Deep Learning | Very High | Very Low |

---

# ğŸ§  Key Insights

- Decision trees **partition feature space**
- Produce **interpretable rules**
- Single tree â†’ simple but weak
- Many trees â†’ powerful model

---

# ğŸ“Œ One-Line Summary

Decision Trees split the feature space into simple regions to form interpretable decision rules ğŸŒ³
