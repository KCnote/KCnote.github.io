---
layout: post
title: "Entry of Classification"
date: 2026-02-06 00:00:00 +0900
author: kang
categories: [Machince Learning, Classification]
tags: [Machince Learning, Overview, ML, Supervised, Regression, Classification]
pin: false
math: true
mermaid: true
---

# ğŸ¯ Classification â€” Linear Regression vs Logistic Regression

---

## ğŸ“Œ Overview

In **classification problems**, our goal is to predict a **discrete class label** rather than a continuous value.

This post explains:

- Why **Linear Regression** is not suitable for classification âŒ  
- Why **Logistic Regression** is the correct probabilistic model âœ…  
- The mathematical intuition behind both approaches ğŸ“  

---

# ğŸ§  Classification Basics

## ğŸ” What is Classification?

Given:

- Feature vector: $$\mathbf{x}$$  
- Class label: $$y \in C$$  

We learn a function:

$$
f(\mathbf{x}) \in C
$$

Often, we prefer **probabilities** instead of hard labels:

$$
P(y = c \mid \mathbf{x})
$$

---

## ğŸ’¡ Why Probabilities Matter

Probabilistic outputs enable:

- ğŸ¯ Riskâ€‘based decision making  
- âš™ï¸ Threshold tuning  
- ğŸ’° Costâ€‘sensitive classification  
- ğŸ“Š Confidence estimation  

Example: Fraud detection â†’ probability is more valuable than a binary decision.

---

# âš ï¸ Can Linear Regression Be Used for Classification?

## Binary Encoding

$$
y =
\begin{cases}
0 & \text{No} \\
1 & \text{Yes}
\end{cases}
$$

One might try:

$$
\hat{y} > 0.5 \Rightarrow \text{Class 1}
$$

---

## ğŸ‘ Why It *Sometimes* Works

Because:

$$
\mathbb{E}[y \mid \mathbf{x}] = P(y=1 \mid \mathbf{x})
$$

Linear regression can approximate probabilities **in limited cases**.

---

## âŒ Major Problems

### 1. Predictions outside [0,1]

Linear regression may produce:

- Negative probabilities âŒ  
- Probabilities > 1 âŒ  

Which is **invalid for probability modeling**.

---

### 2. Multiclass Problem

Numeric coding introduces fake ordering:

$$
1=\text{stroke},\quad 2=\text{overdose},\quad 3=\text{seizure}
$$

Implies meaningless distance relationships â†’ âŒ incorrect structure.

---

## ğŸš« Conclusion

Linear regression is **not suitable for classification**.

Better alternatives:

- Logistic Regression âœ…  
- Softmax / Multinomial Logistic Regression  
- LDA / QDA  
- Probabilistic classifiers  

---

# ğŸ”· Logistic Regression

## ğŸ¯ Goal

Model probability:

$$
P(y=1 \mid \mathbf{x})
$$

We need a function mapping:

$$
(-\infty,+\infty) \rightarrow (0,1)
$$

---

## ğŸ“ˆ Sigmoid Function

$$
\sigma(s) = \frac{1}{1+e^{-s}}
$$

Properties:

- Smooth & monotonic ğŸ“ˆ  
- Valid probability output ğŸ¯  
- Basis of Logistic Regression  

---

## ğŸ“Š Logistic Model

$$
P(y=1 \mid \mathbf{x}) = \frac{1}{1+e^{-\boldsymbol{\beta}^T\mathbf{x}}}
$$

$$
P(y=0 \mid \mathbf{x}) = 1 - P(y=1 \mid \mathbf{x})
$$

---

# ğŸ” Interpretation

## Logâ€‘Odds (Logit)

$$
\log \frac{p}{1-p} = \boldsymbol{\beta}^T \mathbf{x}
$$

Meaning:

- Logistic regression is **linear in logâ€‘odds**, not probability.

---

## Coefficient Meaning

If:

$$
\hat{\beta}_1 > 0
$$

â†’ Increasing feature **increases probability of class 1**.

Each +1 unit change in feature increases **logâ€‘odds by $$\beta_1$$**.

---

# ğŸ“ Maximum Likelihood Estimation (MLE)

## Likelihood

For binary outcome:

$$
P(y_i=1|\mathbf{x}_i)=\sigma(\boldsymbol{\beta}^T\mathbf{x}_i)
$$

Dataset likelihood:

$$
\mathcal{L}(\boldsymbol{\beta})=\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}
$$

---

## Logâ€‘Likelihood

$$
\ell(\boldsymbol{\beta})=\sum_{i=1}^n \left[ y_i \log p_i + (1-y_i)\log(1-p_i) \right]
$$

Equivalent to minimizing **crossâ€‘entropy loss**.

---

# ğŸ§® Linear vs Logistic â€” Key Differences

| Feature | Linear Regression | Logistic Regression |
|--------|------------------|---------------------|
| Output | Real value | Probability (0â€“1) |
| Task | Regression | Classification |
| Valid Probabilities | âŒ | âœ… |
| Decision Boundary | Linear | Linear (in logâ€‘odds) |
| Optimization | Least Squares | MLE / Crossâ€‘Entropy |
| Multiclass Extension | âŒ | Softmax |

---

# ğŸš€ Key Takeaways

- Linear regression can approximate classification but is **not probabilistically valid** âŒ  
- Logistic regression models **true probabilities** using sigmoid âœ…  
- Model is **linear in logâ€‘odds** ğŸ“  
- Estimated using **Maximum Likelihood** ğŸ“Š  
- Foundation of modern classification methods ğŸ§   

---

# ğŸ“š (Optional Extensions)

- ğŸ”¹ Regularization (L1 / L2)  
- ğŸ”¹ Multiclass Softmax Regression  
- ğŸ”¹ Decision boundary geometry  
- ğŸ”¹ Gradient / Hessian derivation  
- ğŸ”¹ Newton / IRLS optimization  
