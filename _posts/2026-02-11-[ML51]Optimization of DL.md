---
layout: post
title: "Optimization"
date: 2026-02-11 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Optimization]
tags: [Artificial Intelligenc, Optimization, Momentum, AdaGrad, RMSProp, Adam]
pin: false
math: true
mermaid: true
---
# ğŸš€ Optimization in Deep Learning  
### From SGD â†’ Momentum â†’ AdaGrad â†’ RMSProp â†’ Adam â†’ Second-Order

---

## ğŸ¯ Why Optimization Matters

We want to minimize:

$$
J(\theta)
$$

by iteratively updating parameters:

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

where:

- $\alpha$ = learning rate  
- $\nabla J(\theta)$ = gradient  

---

# 1ï¸âƒ£ Problems with SGD

## ğŸ“Œ SGD Update

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

---

## âš ï¸ Problem 1: Jittering (Slow Progress)

When curvature differs across directions:

- Oscillation in steep direction  
- Slow movement in flat direction  

This relates to the **condition number** of the Hessian:

$$
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}}
$$

Large $\kappa$ â‡’ slow convergence.

---

## âš ï¸ Problem 2: Local Minima & Saddle Points

At stationary points:

$$
\nabla J(\theta) = 0
$$

Then:

$$
\theta_{t+1} = \theta_t
$$

No update occurs ğŸ˜µ

In high-dimensional space, saddle points are common.

---

## âš ï¸ Problem 3: Noisy Gradient (Mini-batch)

SGD uses approximation:

$$
\nabla J(\theta) \approx \frac{1}{m} \sum_{i=1}^{m} \nabla L_i(\theta)
$$

Small batch â†’ high variance  
Large batch â†’ expensive  

Tradeoff between noise & computation âš–ï¸

---

# 2ï¸âƒ£ SGD + Momentum ğŸï¸

## ğŸ’¡ Idea

Like physics momentum:

$$
\text{momentum} = \text{mass} \times \text{velocity}
$$

We accumulate gradient history.

---

## ğŸ”„ Update Rule

Velocity:

$$
v_{t+1} = \rho v_t - \alpha \nabla J(\theta_t)
$$

Parameter:

$$
\theta_{t+1} = \theta_t + v_{t+1}
$$

where:

- $\rho \approx 0.9$

---

## âœ… Benefits

- Reduces oscillation  
- Accelerates convergence  
- Escapes shallow saddle regions  

---

# 3ï¸âƒ£ AdaGrad ğŸ“Š

## ğŸ’¡ Idea

Adapt learning rate per parameter.

Accumulate squared gradients:

$$
G_t = \sum_{i=1}^{t} g_i^2
$$

Update:

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} g_t
$$

---

## âŒ Weakness

$G_t$ keeps growing:

$$
\frac{\alpha}{\sqrt{G_t}} \to 0
$$

Learning rate shrinks too much ğŸ“‰

---

# 4ï¸âƒ£ RMSProp ğŸ”„

Fixes AdaGrad by exponential moving average.

Running average:

$$
s_t = \beta s_{t-1} + (1 - \beta) g_t^2
$$

Update:

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s_t + \epsilon}} g_t
$$

Typical:

- $\beta = 0.9$

---

# 5ï¸âƒ£ Adam ğŸ”¥

Adaptive Moment Estimation  
Combines Momentum + RMSProp.

---

## ğŸ“Œ First Moment (Mean)

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$

## ğŸ“Œ Second Moment (Variance)

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

Bias correction:

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

$$
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

Final update:

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

---

## ğŸ”¢ Typical Hyperparameters

| Parameter | Value |
|-----------|--------|
| $\beta_1$ | 0.9 |
| $\beta_2$ | 0.999 |
| $\alpha$ | 1e-3 |

---

# 6ï¸âƒ£ First vs Second Order Optimization

---

## ğŸŸ¢ First-Order Methods

Use gradient only:

$$
\theta_{t+1} = \theta_t - \alpha \nabla J(\theta_t)
$$

Examples:

- SGD
- Momentum
- RMSProp
- Adam

âœ… Cheap  
âœ… Scalable  

---

## ğŸ”µ Second-Order Methods

Use Hessian:

$$
H = \nabla^2 J(\theta)
$$

Newton update:

$$
\theta^* = \theta_0 - H^{-1} \nabla J(\theta_0)
$$

---

## ğŸ“ Taylor Expansion

$$
J(\theta) \approx J(\theta_0)
+ (\theta - \theta_0)^T \nabla J(\theta_0)
+ \frac{1}{2} (\theta - \theta_0)^T H (\theta - \theta_0)
$$

More accurate curvature modeling ğŸ¯

---

## âŒ Why Rare in Deep Learning?

If dimension = $d$

- Hessian size: $O(d^2)$  
- Inversion cost: $O(d^3)$  

Modern networks:

$$
d \sim 10^7 \text{ to } 10^9
$$

Too expensive ğŸ’¸

---

# ğŸ§  Summary Table

| Method | Key Idea | Strength | Weakness |
|--------|----------|----------|----------|
| SGD | Raw gradient | Simple | Slow, oscillation |
| Momentum | Velocity | Faster | Needs tuning |
| AdaGrad | Per-dim LR | Good for sparse | LR shrinks too much |
| RMSProp | EMA scaling | Stable | Extra hyperparameter |
| Adam | Momentum + RMSProp | Default choice | Sometimes overfits |

---

# ğŸ“ Practical Tips

- ğŸ”¥ Start with Adam  
- ğŸï¸ Try SGD + Momentum for better generalization  
- ğŸ“‰ Always use LR scheduling  
- ğŸ“Š Monitor validation metric, not just loss  

---

# ğŸ‰ Final Insight

Optimization is not just about minimizing loss.

It determines:

- Convergence speed ğŸš€  
- Stability âš–ï¸  
- Generalization ğŸ“ˆ  

Choosing optimizer = choosing training dynamics.
