---
layout: post
title: "Memory Address Space & Cache"
date: 2026-01-27 00:00:00 +0900
author: kang
categories: [Computer Structure, Memory]
tags: [Computer Structure, Components, FIFO, LRU, TLB, Cache, Memory, Virtual Address, Physical Address]
pin: false
math: true
mermaid: true

---

# ğŸ§  Memory Address Space & Cache

> A clean, **readable, blogâ€‘friendly explanation** of how CPUs handle
> memory addressing, protection, and cache performance.

## ğŸ“Œ Quick Overview

**This post explains:** - How **virtual memory maps to physical
memory** - How the **MMU protects processes** - Why **cache hierarchy
improves performance** - How **locality makes cache effective**

------------------------------------------------------------------------

# ğŸ— PART 1 --- Memory Addressing (Concept Flow)

## 1ï¸âƒ£ Physical vs Virtual Address

### ğŸ§± Physical Address

-   Real **hardware memory location**
-   Used internally by **RAM**
-   **Not visible** to programs

### ğŸ§­ Virtual (Logical) Address

-   Address seen by **CPU & running processes**
-   Each process has **its own private address space**
-   Multiple programs can use the **same address safely**

------------------------------------------------------------------------

## 2ï¸âƒ£ Address Translation --- MMU

The **MMU (Memory Management Unit)** converts **virtual â†’ physical**
addresses.

### ğŸ”„ Translation Model

    Physical Address = Virtual Address + Base Register

### âœ… Why this matters

-   Process isolation
-   Memory safety
-   Prevents illegal access

------------------------------------------------------------------------

## 3ï¸âƒ£ Memory Protection

The CPU ensures a process **cannot access memory outside its region**.

### ğŸ›‘ Limit Register Check

    if address â‰¥ limit â†’ trap (memory fault)

### ğŸ›¡ Benefits

âœ” Process isolation\
âœ” System stability\
âœ” Security enforcement

------------------------------------------------------------------------

# ğŸ§± PART 2 --- Memory Hierarchy & Cache

## 4ï¸âƒ£ Storage Hierarchy Pyramid

> The closer memory is to the CPU, the **faster**, **smaller**, and
> **more expensive** it becomes.

    Registers
    â†“
    L1 Cache
    â†“
    L2 Cache
    â†“
    L3 Cache
    â†“
    RAM
    â†“
    SSD / HDD

------------------------------------------------------------------------

## 5ï¸âƒ£ Cache Memory --- Why It Exists

Cache is a **small, fast SRAM layer** between CPU and RAM.

### ğŸ¯ Purpose

-   Reduce memory latency
-   Store **frequently used data**
-   Prevent CPU stalls

------------------------------------------------------------------------

## 6ï¸âƒ£ Cache Levels & Multiâ€‘Core Design

  Level       Location      Purpose
  ----------- ------------- ------------------
  ğŸŸ¦ **L1**   Inside Core   Fastest access
  ğŸŸ© **L2**   Inside Core   Larger buffer
  ğŸŸ¨ **L3**   Shared        Crossâ€‘core cache

### Split Cache

-   **L1I** â†’ Instruction Cache\
-   **L1D** â†’ Data Cache

------------------------------------------------------------------------

## 7ï¸âƒ£ Cache Coherency (Multiâ€‘Core Problem)

Each CPU core has **its own cache copy**.

### âš  Problem

> Cores may hold **different values**

### âœ… Solution

-   MESI / MOESI coherency protocols
-   Shared L3 cache synchronization

------------------------------------------------------------------------

# ğŸ§  PART 3 --- Why Cache Works (Locality)

## 8ï¸âƒ£ Temporal Locality

> Recently used data is likely used again

Examples: - Loop counters - Function stack data

------------------------------------------------------------------------

## 9ï¸âƒ£ Spatial Locality

> Nearby memory addresses tend to be accessed

Examples: - Arrays - Sequential instructions

------------------------------------------------------------------------

## ğŸ” PART 4 --- Cache Performance

## ğŸ”¹ Cache Hit vs Miss

  Type          Meaning                 Performance
  ------------- ----------------------- -------------
  âœ… **Hit**    Data found in cache     Fast
  âŒ **Miss**   Data fetched from RAM   Slow

### ğŸ“Š Hit Rate Formula

    Hit Rate = Hits / (Hits + Misses)

### ğŸ“ˆ Measuring Cache Performance

Tools: - `perf` - Intel VTune - AMD uProf - ARM PMU

------------------------------------------------------------------------

# ğŸš€ PART 5 --- Advanced Concepts

-   Cache mapping (Direct / Setâ€‘Associative / Fully Associative)
-   Replacement policies (LRU / FIFO)
-   Write policies (Writeâ€‘Back / Writeâ€‘Through)
-   **TLB** --- Address translation cache

------------------------------------------------------------------------
