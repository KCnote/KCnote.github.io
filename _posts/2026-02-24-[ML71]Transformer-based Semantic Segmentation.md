---
layout: post
title: "Semantic Segmentation"
date: 2026-02-24 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Model]
tags: [Artificial Intelligenc, Semantic Segmentation]
pin: false
math: true
mermaid: true

---
---------------------------------------------------------------------
# üß† Transformer-based Semantic Segmentation

## SETR ¬∑ Segmenter ¬∑ DPT -- Complete Mathematical & Architectural Notes

------------------------------------------------------------------------

# 1Ô∏è‚É£ SETR (SEgmentation TRansformer)

üìÑ Paper: https://arxiv.org/abs/2012.15840

## üî∑ Core Idea

First work applying **pure ViT encoder** to semantic segmentation.

Instead of CNN encoder + decoder, SETR uses:

$$
\textbf{Image} \rightarrow \textbf{Patch Embedding} \rightarrow \textbf{Transformer Encoder} \rightarrow \textbf{Light Decoder}
$$

------------------------------------------------------------------------

## üî∑ Patch Embedding

Input:

$$
X \in \mathbb{R}^{H \times W \times C}
$$

Split into patches of size:

$$
P \times P
$$

Number of patches:

$$
N = \frac{HW}{P^2}
$$

Flatten each patch:

$$
x_i \in \mathbb{R}^{P^2C}
$$

Linear projection:

$$
z_i = W_E x_i
$$

Where:

$$
W_E \in \mathbb{R}^{D \times (P^2C)}
$$

Add positional encoding:

$$
z_i^{(0)} = z_i + p_i
$$

------------------------------------------------------------------------

## üî∑ Transformer Encoder

For layer ‚Ñì:

Self-attention:

$$
\text{Attention}(Q,K,V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{D}}\right)V
$$

With:

$$
Q = ZW_Q,\quad K = ZW_K,\quad V = ZW_V
$$

Stack L layers:

$$
Z_L \in \mathbb{R}^{N \times D}
$$

Each token contains **global contextualized information**.

------------------------------------------------------------------------

# 2Ô∏è‚É£ SETR Decoders

### üü¢ Naive Decoder

-   1√ó1 Conv
-   Bilinear upsampling
-   Pixel-wise cross-entropy

### üîµ Progressive Upsampling (PUP)

Alternating:

Conv ‚Üí Upsample ‚Üí Conv ‚Üí Upsample

Gradually restore resolution.

### üî¥ Multi-Level Aggregation (MLA)

Use features from multiple layers:

$$
Z^{(6)}, Z^{(12)}, Z^{(18)}, Z^{(24)}
$$

Since same size:

$$
Z_{agg} = \sum_i Z^{(i)}
$$

Then upsample.

Best performance: MLA \> PUP \> Naive

------------------------------------------------------------------------

# 3Ô∏è‚É£ Segmenter

üìÑ Paper: https://arxiv.org/abs/2105.05633

## üî∑ Encoder

Standard ViT:

$$
Z_L \in \mathbb{R}^{N \times D}
$$

------------------------------------------------------------------------

## üî∑ Mask Transformer Decoder

Introduce:

$$
K \text{ learnable class embeddings}
$$

Append to patch tokens:

$$
Z' = [Z_L ; C]
$$

Run another Transformer.

Compute mask logits via dot-product:

$$
M = Z_L C^T
$$

Where:

$$
M \in \mathbb{R}^{N \times K}
$$

Reshape to 2D and upsample.

Apply softmax over classes.

------------------------------------------------------------------------

# üîé Impact of Patch Size

Smaller patch size:

-   Better spatial precision
-   Higher memory
-   Higher compute

Trade-off:

  Patch Size   Precision   Compute
  ------------ ----------- ----------
  32√ó32        Low         Fast
  16√ó16        Medium      Balanced
  8√ó8          High        Heavy

------------------------------------------------------------------------

# 4Ô∏è‚É£ DPT (Dense Prediction Transformer)

üìÑ Paper: https://arxiv.org/abs/2103.13413

## üî∑ Core Difference

Instead of single-scale tokens:

Reassemble features at multiple resolutions.

------------------------------------------------------------------------

## üî∑ Reassemble Block

Transform token sequence:

$$
Z \in \mathbb{R}^{N \times D}
$$

Back to spatial grid:

$$
Z \rightarrow F \in \mathbb{R}^{H' \times W' \times D}
$$

Project via convolution to different scales.

Multi-scale fusion similar to FPN.

------------------------------------------------------------------------

## üî∑ CLS Token Handling

Options:

-   Ignore
-   Add to features
-   Project via MLP

------------------------------------------------------------------------

# 5Ô∏è‚É£ Applications

‚úî Semantic segmentation\
‚úî Depth estimation\
‚úî Dense prediction tasks

------------------------------------------------------------------------

# 6Ô∏è‚É£ Comparison

  Model       Encoder   Decoder Type         Multi-scale   Global Context
  ----------- --------- -------------------- ------------- ----------------
  SETR        ViT       Simple / PUP / MLA   ‚ùå            ‚úÖ
  Segmenter   ViT       Mask Transformer     ‚ùå            ‚úÖ
  DPT         ViT       Multi-scale fusion   ‚úÖ            ‚úÖ

------------------------------------------------------------------------

# 7Ô∏è‚É£ Mathematical Insight

CNN segmentation:

$$
\text{Local receptive field}
$$

Transformer segmentation:

$$
\text{Global receptive field}
$$

Self-attention complexity:

$$
\mathcal{O}(N^2)
$$

Where:

$$
N = \frac{HW}{P^2}
$$

Thus smaller patches ‚Üí quadratic explosion.

------------------------------------------------------------------------

# üî• Key Takeaways

1.  SETR proved ViT works for segmentation.
2.  Segmenter introduced class embeddings for masks.
3.  DPT solved multi-scale issue.
4.  Patch size controls resolution vs compute.
5.  Transformer gives global context without convolution.
