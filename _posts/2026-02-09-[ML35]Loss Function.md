---
layout: post
title: "Loss Function"
date: 2026-02-09 00:00:00 +0900
author: kang
categories: [Machince Learning, Optimization]
tags: [Machince Learning, Optimization, Mathematics, SVM, Margin]
pin: false
math: true
mermaid: true

---

# ğŸ“˜ Margin-Based Loss Functions

---

## ğŸ¯ What is a Loss Function?

A loss function measures **how wrong** a model prediction is.

Given prediction $\hat{y}$ and label $y$:

$$
\mathcal{L}(\hat{y}, y)
$$

- If $\hat{y} = y$ â†’ small loss (â‰ˆ0)
- If $\hat{y} \ne y$ â†’ larger loss
- Larger error â†’ larger penalty

---

## ğŸ§  Binary Classification Setup

Labels:

$$
y \in \{-1, +1\}
$$

Model outputs a **score**:

$$
f(x) = w^T x + b
$$

Prediction:

$$
\hat{y} = \text{sign}(f(x))
$$

Define **margin**:

$$
m = y f(x) = y(w^T x + b)
$$

- $m > 0$ â†’ correct classification
- $m < 0$ â†’ wrong classification

Loss depends only on **margin $m$** â†’ Marginâ€‘based loss

---

## ğŸ“‰ 0/1 Loss

Definition:

$$
\mathcal{L}_{0/1}(m) =
\begin{cases}
0 & m > 0 \\
1 & m \le 0
\end{cases}
$$

Nonâ€‘differentiable â†’ hard to optimize.

---

## ğŸ“ˆ Logistic (Log) Loss â€” Full Derivation

Logistic model:

$$
P(y=1|x) = \sigma(f(x)) = \frac{1}{1 + e^{-f(x)}}
$$

Likelihood:

$$
P(y|x) = \sigma(f(x))^{\frac{1+y}{2}} (1-\sigma(f(x)))^{\frac{1-y}{2}}
$$

Negative log-likelihood:

$$
\mathcal{L}_{log}(m)
= \log(1 + e^{-m})
$$

Properties:

- Smooth & differentiable
- Probabilistic interpretation
- Penalizes wrong predictions smoothly

Gradient:

$$
\frac{d}{dm}\log(1+e^{-m}) = -\frac{1}{1+e^{m}}
$$

---

## ğŸ“Š Exponential Loss â€” AdaBoost

Used in AdaBoost:

$$
\mathcal{L}_{exp}(m) = e^{-m}
$$

Derivation (Boosting objective):

Minimize weighted classification error:

$$
\sum_i e^{-y_i f(x_i)}
$$

Properties:

- Very sensitive to outliers
- Large penalty for misclassified points

---

## ğŸ“ Hinge Loss â€” SVM

Definition:

$$
\mathcal{L}_{hinge}(m) = \max(0, 1 - m)
$$

From Softâ€‘Margin SVM:

Primal problem:

$$
\min_w \frac{1}{2}\|w\|^2 + C\sum_i \xi_i
$$

with:

$$
\xi_i = \max(0, 1 - y_i f(x_i))
$$

Substitute â†’

$$
\min_w \frac{1}{2}\|w\|^2 + C\sum_i \max(0, 1 - y_i f(x_i))
$$

ğŸ‘‰ SVM = **Hinge Loss + L2 Regularization**

---

## ğŸ“ Geometric Interpretation

Margin:

$$
m = y f(x)
$$

- $m \ge 1$ â†’ no loss
- $0 < m < 1$ â†’ inside margin
- $m < 0$ â†’ misclassified

---

## ğŸ”¬ Comparison of Loss Functions

| Loss | Formula | Smooth | Robust to Noise |
|------|--------|--------|-----------------|
| 0/1 | $\mathbf{1}(m \le 0)$ | âŒ | Medium |
| Logistic | $\log(1+e^{-m})$ | âœ” | Good |
| Exponential | $e^{-m}$ | âœ” | âŒ Sensitive |
| Hinge | $\max(0,1-m)$ | Piecewise | Good |

---

## âš–ï¸ Logistic vs Hinge vs Exponential

### Logistic

- Smooth optimization
- Probabilistic
- Robust

### Hinge

- Sparse solution (Support Vectors)
- Margin maximization
- Efficient

### Exponential

- Strong focus on hard samples
- Sensitive to outliers

---

## ğŸ”¥ Unified Risk Minimization

All margin-based classifiers minimize:

$$
\min_w \frac{\lambda}{2}\|w\|^2 + \sum_i \mathcal{L}(y_i f(x_i))
$$

Different loss â†’ different algorithm.

---

## ğŸŒˆ Shapes of Loss Functions

- 0/1 â†’ step
- Logistic â†’ smooth Sâ€‘curve
- Exponential â†’ steep penalty
- Hinge â†’ piecewise linear

---

## ğŸš€ Summary

- Loss depends on margin $m = y f(x)$
- Logistic â†’ probabilistic smooth loss
- Exponential â†’ boosting loss
- Hinge â†’ SVM loss
- All are special cases of **regularized risk minimization**
