---
layout: post
title: "Activation Functions"
date: 2026-02-11 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Optimization]
tags: [Artificial Intelligence, Optimization]
pin: false
math: true
mermaid: true
---

# ğŸ§  1. Why Activation Functions Matter

Without activation:

$$
f(x) = W_2(W_1 x)
$$

This simplifies to:

$$
f(x) = (W_2 W_1)x
$$

âŒ Still linear.\
âŒ Cannot model complex patterns.

With activation:

$$
f(x) = W_2 a(W_1 x)
$$

âœ… Introduces non-linearity\
âœ… Enables deep learning power

------------------------------------------------------------------------

# ğŸ”µ 2. Sigmoid Function

## ğŸ“Œ Definition

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

Range:

$$
0 < \sigma(x) < 1
$$

------------------------------------------------------------------------

## ğŸ§® Full Derivative Derivation

Start:

$$
\sigma(x) = (1 + e^{-x})^{-1}
$$

Differentiate:

$$
\frac{d}{dx}\sigma(x)
= -1(1+e^{-x})^{-2}(-e^{-x})
$$

$$
= \frac{e^{-x}}{(1+e^{-x})^2}
$$

Rewrite:

$$
= \frac{1}{1+e^{-x}}\left(1 - \frac{1}{1+e^{-x}}\right)
$$

Therefore:

$$
\sigma'(x) = \sigma(x)(1-\sigma(x))
$$

------------------------------------------------------------------------

## âš ï¸ Vanishing Gradient

If $x \to +\infty$:

$$
\sigma'(x) \to 0
$$

If $x \to -\infty$:

$$
\sigma'(x) \to 0
$$

ğŸš¨ Gradients vanish in deep networks.

------------------------------------------------------------------------

## âš ï¸ Not Zero-Centered

$$
\sigma(x) > 0
$$

All outputs positive â†’ inefficient zig-zag gradient updates.

------------------------------------------------------------------------

# ğŸŸ¢ 3. Tanh Function

## ğŸ“Œ Definition

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

Relation:

$$
\tanh(x) = 2\sigma(2x) - 1
$$

Range:

$$
-1 < \tanh(x) < 1
$$

------------------------------------------------------------------------

## ğŸ§® Derivative

$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x)
$$

âœ” Zero-centered\
âŒ Still saturates for large $|x|$

------------------------------------------------------------------------

# ğŸ”´ 4. ReLU (Rectified Linear Unit)

## ğŸ“Œ Definition

$$
\text{ReLU}(x) = \max(0,x)
$$

Piecewise:

$$
\text{ReLU}(x) =
\begin{cases}
x & x > 0 \\
0 & x \le 0
\end{cases}
$$

------------------------------------------------------------------------

## ğŸ§® Derivative

$$
\text{ReLU}'(x) =
\begin{cases}
1 & x > 0 \\
0 & x < 0
\end{cases}
$$

âœ” No saturation for $x>0$\
âœ” Fast computation\
âœ” Sparse activation

------------------------------------------------------------------------

## âš ï¸ Dead ReLU

If neuron always outputs 0:

$$
\nabla = 0
$$

Weights stop updating âŒ

------------------------------------------------------------------------

# ğŸŸ¡ 5. Leaky ReLU

## ğŸ“Œ Definition

$$
f(x) =
\begin{cases}
x & x > 0 \\
\alpha x & x \le 0
\end{cases}
$$

------------------------------------------------------------------------

## ğŸ§® Derivative

$$
f'(x) =
\begin{cases}
1 & x > 0 \\
\alpha & x \le 0
\end{cases}
$$

âœ” Prevents dead neurons\
âœ” Keeps small gradient for negative region

------------------------------------------------------------------------

# ğŸŸ£ 6. ELU (Exponential Linear Unit)

## ğŸ“Œ Definition

$$
\text{ELU}(x) =
\begin{cases}
x & x \ge 0 \\
\alpha(e^x - 1) & x < 0
\end{cases}
$$

------------------------------------------------------------------------

## ğŸ§® Derivative

For $x \ge 0$:

$$
\frac{d}{dx} = 1
$$

For $x < 0$:

$$
\frac{d}{dx} = \alpha e^x
$$

âœ” Closer to zero-centered\
âœ” Smooth negative region

------------------------------------------------------------------------

# ğŸ“Š 7. Comparison Table

  Activation   Saturation   Zero-Centered   Vanishing Gradient
  ------------ ------------ --------------- --------------------
  Sigmoid      Yes          No              Severe
  Tanh         Yes          Yes             Moderate
  ReLU         No (+side)   No              Partial
  Leaky ReLU   No           No              Minimal
  ELU          Mild         Closer          Minimal

------------------------------------------------------------------------

# ğŸ¯ 8. Final Practical Advice

âœ… Use **ReLU** by default\
âœ… Try **Leaky ReLU / ELU** for improvements\
âŒ Avoid Sigmoid/Tanh in deep hidden layers

------------------------------------------------------------------------

# ğŸš€ Core Insight

Activation functions control:

$$
\text{Non-linearity}
$$

$$
\text{Gradient flow}
$$

$$
\text{Optimization stability}
$$

They are fundamental to deep learning success.
