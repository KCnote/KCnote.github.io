---
layout: post
title: "Nearest Neighbor"
date: 2026-02-10 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Model]
tags: [Artificial Intelligence, Model, Nearest Neighbor]
pin: false
math: true
mermaid: true
---

# ğŸ–¼ï¸ Image Classification & Nearest Neighbor

> Supervised Learning Basics â€“ Image Classification  
> Lecture-style notes with full mathematical details âœ¨

---

## ğŸ§  An Image Classifier

Goal:
$$
f(\text{image}) \rightarrow \text{class label}
$$

There is no rule-based way to hard-code image recognition.

---

## ğŸ¤– Machine Learning: Data-driven Approach

1. Collect labeled images  
2. Train a classifier  
3. Predict unseen images  

$$
\text{model} = \text{train}(\text{images}, \text{labels})
$$

$$
\hat{y} = \text{predict}(\text{model}, \text{image})
$$

---

## ğŸ” Nearest Neighbor Classification

- Store all training images
- Assign the label of the most similar image

---

## ğŸ“ Distance Metrics

Images are vectorized:
$$
A, B \in \mathbb{R}^d
$$

### L1 Distance
$$
L_1(A,B) = \sum_i |A_i - B_i|
$$

### L2 Distance
$$
L_2(A,B) = \sqrt{\sum_i (A_i - B_i)^2}
$$

---

## âš™ï¸ Algorithm Complexity

- Training: $$O(1)$$
- Prediction: $$O(N)$$

---

## ğŸ‘¥ k-Nearest Neighbor

- Use majority vote from k closest points
- Larger k â†’ smoother boundary

---

## âš ï¸ Issues

- Curse of dimensionality
- Pixel distance not semantic
- Slow at test time

---

## ğŸ› ï¸ Practical Tips

- Normalize features
- Reduce dimensionality
- Tune k using validation
- Use approximate NN

---

## ğŸ§  Summary

Nearest Neighbor is simple but rarely used directly on pixels.
