---
layout: post
title: "RNN - 2"
date: 2026-02-11 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Model]
tags: [Artificial Intelligenc, Model, RNN]
pin: false
math: true
mermaid: true
---
# ğŸ“š Language Models & RNNs

---

# ğŸŒ 1. What is a Language Model?

## ğŸ—£ What is Language?

Language is a structured sequence of words used for communication.

---

## ğŸ¤– What is a Language Model?

A language model assigns probability to a word sequence:

$$
p(w_1, w_2, \dots, w_T)
$$

It distinguishes natural sentences from unnatural ones.

Example:

$$
p(\text{"I study machine learning"}) 
>
p(\text{"Machine I learning study"})
$$

---

# ğŸ“œ 2. Traditional Language Models: n-grams

## ğŸ”¹ Chain Rule

$$
P(w_1, \dots, w_m) 
= 
\prod_{i=1}^{m} P(w_i \mid w_1, \dots, w_{i-1})
$$

---

## ğŸ”¹ Markov Assumption (n-gram)

Instead of conditioning on all previous words:

$$
P(w_i \mid w_1, \dots, w_{i-1})
\approx
P(w_i \mid w_{i-n}, \dots, w_{i-1})
$$

---

## ğŸ”¹ Bigram Example

$$
p(w_2 \mid w_1) 
=
\frac{\text{count}(w_1, w_2)}
{\text{count}(w_1)}
$$

---

## âŒ Problem of n-grams

- Memory grows exponentially
- Requires large dataset
- Cannot model long-range dependencies

---

# ğŸ” 3. RNN for Language Modeling

## ğŸ¤” Why RNN?

- Hidden state stores past information
- Same weights are reused at every time step
- Memory does NOT grow with sentence length

---

## ğŸ”¹ RNN Recurrence

$$
h_t = f_W(h_{t-1}, x_t)
$$

Specifically:

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t)
$$

Where:

- $W_{hh}$: hidden-to-hidden weights  
- $W_{xh}$: input-to-hidden weights  

---

# ğŸ¯ 4. Predicting the Next Word

We model:

$$
P(w_t \mid w_1, \dots, w_{t-1})
$$

This becomes a multiclass classification problem over vocabulary.

---

## ğŸ”¹ Output Layer

$$
\hat{y}_t = \text{softmax}(W_{hy} h_t)
$$

Where:

- $W_{hy}$: hidden-to-output weights  
- $\hat{y}_t \in \mathbb{R}^{|V|}$  
- $|V|$ = vocabulary size  

---

## ğŸ”¹ Cross-Entropy Loss

For one time step:

$$
\mathcal{L}^{(t)} 
= 
- \sum_{j=1}^{|V|} 
y_{t,j} \log \hat{y}_{t,j}
$$

Total loss:

$$
\mathcal{L}
=
\sum_{t=1}^{T}
\mathcal{L}^{(t)}
$$

---

# ğŸ”„ 5. Many-to-Many RNN

Instead of predicting only at the final step,  
predict at every time step.

Binary classification:

$$
\hat{y}_t = \sigma(W_{hy} h_t)
$$

Regression:

$$
\hat{y}_t = W_{hy} h_t
$$

---

# ğŸ§± 6. Multi-Layer RNN

Stack multiple RNN layers:

$$
h_t^{(l)}
=
\tanh
\left(
W_{hh}^{(l)} h_{t-1}^{(l)}
+
W_{xh}^{(l)} h_t^{(l-1)}
\right)
$$

---

# ğŸ”¥ 7. PyTorch Vanilla RNN Example

```python
import torch
import torch.nn as nn

rnn = nn.RNN(
    input_size=10,
    hidden_size=20,
    num_layers=3
)

input = torch.randn(5, 64, 10)   # (T, batch, input_size)
h0 = torch.randn(3, 64, 20)      # (num_layers, batch, hidden_size)

output, hn = rnn(input, h0)
```

---

## ğŸ“ Dimensions

- Sequence length: $T = 5$
- Batch size: $64$
- Input dimension: $10$
- Hidden dimension: $20$
- Layers: $3$

---

# ğŸ§  Summary

| Model   | Memory Usage | Long-range Modeling |
|----------|-------------|--------------------|
| n-gram   | âŒ exponential | âŒ poor |
| RNN      | âœ… constant | âš  limited |
| Deep RNN | âœ… constant | âš  better |

---
