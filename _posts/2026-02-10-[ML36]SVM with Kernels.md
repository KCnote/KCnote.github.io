---
layout: post
title: "Support Vector Machines with Kernels"
date: 2026-02-10 00:00:00 +0900
author: kang
categories: [Machince Learning, Model]
tags: [Machince Learning, Model, Mathematics, SVM, Margin, Kernel]
pin: false
math: true
mermaid: true

---

# ðŸ“˜ Support Vector Machines â€” Kernels, Logistic Comparison, and Multiâ€‘Class

---

## âš”ï¸ SVM vs Logistic Regression

### When classes are well separated

SVM tends to perform better because it **maximizes the margin**:

$$
\max \frac{2}{\|w\|}
$$

Logistic regression keeps pushing probabilities toward 0/1 and may become unstable when perfectly separable (weights â†’ âˆž).

---

### When classes overlap (noisy data)

Logistic regression often performs similarly or better because it minimizes **log loss**:

$$
\sum \log(1 + e^{-y_i f(x_i)})
$$

while SVM minimizes hinge loss:

$$
\sum \max(0, 1 - y_i f(x_i))
$$

---

### Probabilities

Logistic regression outputs:

$$
P(y=1|x) = \frac{1}{1 + e^{-f(x)}}
$$

SVM â†’ only decision boundary (no probability unless calibrated).

---

### Nonlinear boundaries

Kernel SVM naturally handles nonlinear boundaries using kernel trick.  
Kernel logistic regression exists but is computationally heavier.

---

## ðŸŒŒ Linearly Nonâ€‘Separable Data â†’ Kernel Trick

We map data into higher dimension:

$$
x \rightarrow \phi(x)
$$

Linear classifier becomes:

$$
f(x) = w^T \phi(x) + b
$$

Instead of explicit mapping, use kernel:

$$
K(x_i, x_j) = \phi(x_i)^T \phi(x_j)
$$

---

## ðŸ“ Inner Products & Support Vectors

From SVM dual solution:

$$
w = \sum_{i=1}^{n} \alpha_i y_i x_i
$$

Prediction:

$$
f(x) = \text{sign} \left( \sum_{i=1}^{n} \alpha_i y_i x_i^T x + b \right)
$$

Only $\alpha_i > 0$ contribute â†’ **Support Vectors**.

---

## ðŸ§  Kernelized SVM

Replace inner product:

$$
x_i^T x_j \rightarrow K(x_i, x_j)
$$

Decision function:

$$
f(x) = \text{sign} \left( \sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b \right)
$$

---

## ðŸ“Š Polynomial Kernel

$$
K(x_i, x_j) = (1 + x_i^T x_j)^d
$$

Implicitly maps data to **all monomials up to degree $d$**.

---

## ðŸŒ Radial Basis Function (RBF / Gaussian Kernel)

$$
K(x_i, x_j) = \exp\left(-\gamma \|x_i - x_j\|^2\right)
$$

Properties:

- Local similarity measure
- Small $\gamma$ â†’ smooth boundary
- Large $\gamma$ â†’ complex / wiggly boundary

---

## ðŸ” Why Kernel Works (Derivation)

From dual optimization:

$$
\max_{\alpha}
\sum \alpha_i - \frac{1}{2}\sum\sum \alpha_i \alpha_j y_i y_j x_i^T x_j
$$

Replace inner product:

$$
x_i^T x_j = \phi(x_i)^T \phi(x_j) = K(x_i,x_j)
$$

Thus SVM works in highâ€‘dim space **without explicit mapping**.

---

## ðŸ‘¥ SVM for Multiâ€‘Class (K > 2)

SVM is inherently binary â†’ need strategy.

### Oneâ€‘vsâ€‘Rest (OvR)

Train K classifiers:

$$
f_k(x) = w_k^T x + b_k
$$

Prediction:

$$
\hat{y} = \arg\max_k f_k(x)
$$

---

### Oneâ€‘vsâ€‘One (OvO)

Train pairwise classifiers:

$$
\binom{K}{2}
$$

Final prediction â†’ majority voting.

Preferred when K is small.

---

## ðŸ”¬ Geometric Interpretation

Kernel transforms space so that nonlinear boundary in original space becomes **linear hyperplane in feature space**.

---

## ðŸ“Š Logistic vs SVM â€” Mathematical Difference

| Method | Objective |
|--------|----------|
| Logistic | $\sum \log(1 + e^{-y f(x)})$ |
| SVM | $\frac{1}{2}\|w\|^2 + C\sum \max(0, 1-yf(x))$ |

Logistic â†’ probabilistic model  
SVM â†’ margin maximization

---

## ðŸš€ Final Summary

- SVM maximizes margin â†’ strong geometric classifier
- Logistic gives probabilities â†’ probabilistic interpretation
- Kernel trick enables nonlinear boundaries
- RBF kernel â†’ local similarity
- Multiâ€‘class via OvR / OvO
- Support vectors define decision boundary

---

**End of Notes âœ¨**
