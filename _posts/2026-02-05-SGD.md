---
layout: post
title: "Stochastic Gradient Descent"
date: 2026-02-05 00:00:00 +0900
author: kang
categories: [Machince Learning, Optimization]
tags: [Machince Learning, Overview, ML, Supervised, Classification, MLE, Optimization, Gradient Descent]
pin: false
math: true
mermaid: true
---

# âš¡ Stochastic Gradient Descent (SGD)

> ğŸ“˜ Clean blog-ready version with visual structure and emojis  
> ğŸ¯ Covers core idea, variants, batch size effects, and practical insights  

---

# ğŸš€ Core Idea

Instead of computing gradients using **all training examples**, SGD computes gradients using a **randomly sampled subset** of the data.

This greatly improves speed and scalability for large datasets.

---

# ğŸ”€ Variants of SGD

## 1ï¸âƒ£ Pure Stochastic Gradient Descent

- Updates parameters using **one single sample at a time**
- Very fast updates âš¡  
- But gradients are **very noisy** ğŸŒªï¸  
- May oscillate around the optimum  

---

## 2ï¸âƒ£ Miniâ€‘Batch Gradient Descent (Most Common)

- Uses a **mini-batch** of samples to estimate the gradient
- Typical batch sizes:

$$
32,\; 64,\; 128,\; 256,\; \dots,\; 8192
$$

The optimal batch size depends on:

- Problem characteristics ğŸ§   
- Dataset size ğŸ“Š  
- Hardware constraints (especially memory) ğŸ’»  

---

# ğŸ“‰ Effect of Batch Size

## Small Miniâ€‘Batch

- Doubling batch size â†’ **significant gradient stabilization**
- Noise reduced â†’ smoother convergence

## Large Miniâ€‘Batch

- Improvement becomes smaller  
- Computation cost increases roughly linearly  
- Known as **diminishing returns**  

---

# âœ… Advantages of SGD

- Much faster than full gradient descent for large datasets âš¡  
- Scales well to massive data ğŸŒ  
- Noise can help escape shallow local minima and saddle points ğŸ§—  

---

# ğŸ› ï¸ Practical Notes

Miniâ€‘batch SGD is the **standard optimization method** in deep learning.

Often combined with:

- Momentum ğŸš€  
- Adam âš™ï¸  
- RMSProp ğŸ“ˆ  
- Learning rate scheduling â±ï¸  

These techniques improve convergence speed and stability.

---

# ğŸ¯ Key Insight

- SGD trades **exact gradient** for **speed and scalability**
- Miniâ€‘batch provides a balance between:
  - Stability ğŸ“Š  
  - Speed âš¡  
- Widely used in modern machine learning and deep learning
