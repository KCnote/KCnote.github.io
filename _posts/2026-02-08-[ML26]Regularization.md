---
layout: post
title: "Regularization"
date: 2026-02-08 00:00:00 +0900
author: kang
categories: [Machince Learning, Fitting]
tags: [Machince Learning, Mathematics, ML, Supervised, Fitting, Overfitting, Underfitting, Capacity, Regularization, Ridge, Lasso]
pin: false
math: true
mermaid: true
---

# ğŸ“˜ Regularization, Overfitting, Ridge & Lasso

---

## ğŸ¯ Goal of Learning â€” Generalization

The objective of machine learning is **not to minimize training error**, but to **generalize well to unseen data**.

Typical behavior:

$$
\text{Training loss} \downarrow,\quad \text{Validation loss} \uparrow
$$

âš ï¸ This indicates **Overfitting** â€” the model starts memorizing noise instead of learning true patterns.

---

## ğŸ“‰ Small Data Problem (e.g., Video Summarization)

Some problems have extremely limited data:

- ğŸ¬ Video summarization  
- ğŸ§¬ Medical / rare events  
- ğŸ’¸ Expensive labeling  

Even collecting **30 samples** can be costly.

### Risks
- Crossâ€‘validation may become **misleading**
- Overfitting is hard to detect
- High variance in evaluation

### Practical Strategy
- âœ” Strict validation  
- âœ” Train **multiple capacity models**  
- âœ” Estimate overfitting onset  
- âœ” Avoid trusting a single run  

---

## â¹ï¸ Early Stopping

Under i.i.d. assumption, overfitting tends to begin at similar times across splits.

$$
\text{Stop at } \arg\min_{\text{epoch}} \text{Validation Metric}
$$

âš ï¸ Notes:

- Surrogate loss â‰  true metric  
- Metrics may overfit at different points  
- Use **most important real metric**  

---

## ğŸ§  Model Capacity

| Capacity | Behavior |
|----------|----------|
| ğŸ”¹ Low | Underfitting |
| â­ Optimal | Best Generalization |
| ğŸ”º High | Overfitting |

Biasâ€“Variance:

| Capacity | Bias | Variance |
|----------|------|----------|
| Low | High | Low |
| Optimal | Balanced | Balanced |
| High | Low | High |

---

## ğŸ“ Regularization / Shrinkage

General form:

$$
\min_{\beta} RSS + \lambda \cdot \text{Penalty}(\beta)
$$

Goal:

$$
\text{Reduce variance by shrinking coefficients } \beta
$$

---

## ğŸ”µ Ridge Regression (L2)

Objective:

$$
\hat{\beta} = \arg\min_\beta \sum_{i=1}^{n}(y_i - x_i^T\beta)^2 + \lambda \|\beta\|_2^2
$$

Matrix solution:

$$
\hat{\beta} = (X^T X + \lambda I)^{-1} X^T Y
$$

### Properties
- Shrinks coefficients toward 0  
- Rarely exactly 0  
- Keeps all features  
- Variance â†“, Bias â†‘  

Limits:

$$
\lambda = 0 \Rightarrow \text{OLS}
$$

$$
\lambda \to \infty \Rightarrow \beta \to 0
$$

---

## âš–ï¸ Scaling of Predictors (Critical)

Ridge is **scale sensitive**, therefore predictors must be standardized:

$$
\tilde{x}_{ij} = \frac{x_{ij} - \bar{x}_j}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\bar{x}_j)^2}}
$$

Why?

Penalty term:

$$
\lambda \sum_{j=1}^{p} \beta_j^2
$$

- Largeâ€‘scale feature â†’ Larger penalty âŒ  
- Smallâ€‘scale feature â†’ Smaller penalty âŒ  

âœ” Scaling ensures **fair regularization**.

---

## ğŸ“Š Biasâ€“Variance (Ridge Insight)

$$
\text{MSE} = \text{Bias}^2 + \text{Variance} + \sigma^2
$$

As $$\lambda$$ increases:

- Variance â†“  
- Bias â†‘  
- Optimal $$\lambda$$ exists â­  

---

## ğŸ”¶ Lasso (L1)

Objective:

$$
\hat{\beta} = \arg\min_\beta \sum_{i=1}^{n}(y_i - x_i^T\beta)^2 + \lambda \|\beta\|_1
$$

$$
\|\beta\|_1 = \sum_{j=1}^{p} |\beta_j|
$$

### Properties
- Some coefficients become **exactly 0**  
- Produces **Sparse Model**  
- Performs feature selection  

---

## ğŸ’ Why Lasso Produces Zeros

Constraint:

$$
\|\beta\|_1 \le t
$$

Diamond geometry â†’ solutions at corners:

$$
\beta_j = 0
$$

âœ” Natural feature selection

---

## ğŸŒ Highâ€‘Dimensional Effect

Higher dimension â†’ more corners â†’ more zeros â†’ **sparser model**

---

## âš”ï¸ Ridge vs Lasso

| Property | Ridge ğŸ”µ | Lasso ğŸ”¶ |
|----------|----------|----------|
| Penalty | $$L_2$$ | $$L_1$$ |
| Coefficients | Close to 0 | Can be exactly 0 |
| Feature selection | âŒ | âœ” |
| Stability | High | Medium |
| Sparse model | âŒ | âœ” |

---

## ğŸ¤” Which is Better?

No universal winner.

- Sparse truth â†’ Lasso better  
- Dense truth â†’ Ridge better  

Best practice:

$$
\text{Choose using Cross Validation}
$$

---

## ğŸ›ï¸ Hyperparameter Selection

1. Choose grid of $$\lambda$$  
2. Compute CV error  
3. Select minimum  
4. Refit with all data  

---

## ğŸ§© Practical Insights

- Small data â†’ CV may deceive âš ï¸  
- Always scale predictors âœ”  
- Ridge â†’ Smooth shrinkage  
- Lasso â†’ Sparse solution  
- Capacity tuning critical  
- Estimate overfitting onset  

---

## ğŸŒŸ Final Insight

A good model is **not the one with lowest training loss**,  
but the one achieving **best generalization with controlled complexity**.


----
----

# Ridge Regression â€” Matrix Derivation

---

## Objective Function

Ridge regression minimizes:

$$
\hat{\beta} = \arg\min_{\beta} (Y - X\beta)^T (Y - X\beta) + \lambda \|\beta\|_2^2
$$

---

## Define the Objective

$$
J(\beta) = (Y - X\beta)^T (Y - X\beta) + \lambda \beta^T \beta
$$

---

## Gradient w.r.t. $$\beta$$

Taking derivative:

$$
\frac{\partial RSS(\beta)}{\partial \beta}
= -X^T (Y - X\beta) - (Y - X\beta)^T X + 2\lambda \beta
$$

Using transpose property:

$$
(Y - X\beta)^T X = X^T (Y - X\beta)
$$

So:

$$
\frac{\partial RSS(\beta)}{\partial \beta}
= -X^T (Y - X\beta) - X^T (Y - X\beta) + 2\lambda \beta
$$

---

## Combine Terms

$$
\frac{\partial RSS(\beta)}{\partial \beta}
= -2X^T (Y - X\beta) + 2\lambda \beta
$$

---

## Set Gradient to Zero

$$
-2X^T (Y - X\beta) + 2\lambda \beta = 0
$$

Divide by 2:

$$
X^T (Y - X\beta) = \lambda \beta
$$

---

## Expand

$$
X^T Y - X^T X \beta = \lambda \beta
$$

---

## Rearrangement

$$
X^T Y - X^T X \beta - \lambda \beta = 0
$$

$$
X^T Y = (X^T X + \lambda I)\beta
$$

---

## Closed-form Solution

$$
\hat{\beta} = (X^T X + \lambda I)^{-1} X^T Y
$$

---

## Dimension Check

$$
\beta \in \mathbb{R}^{d \times 1}
$$

$$
X^T X \in \mathbb{R}^{d \times d}
$$

$$
X^T Y \in \mathbb{R}^{d \times 1}
$$

Therefore:

$$
(X^T X + \lambda I)^{-1} X^T Y \in \mathbb{R}^{d \times 1}
$$

âœ” Dimensions match â€” valid solution.
