---
layout: post
title: "Batch Normalization"
date: 2026-02-11 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Optimization]
tags: [Artificial Intelligenc, Batch, Normalization]
pin: false
math: true
mermaid: true
---
# ğŸ“Š Data Preprocessing & Batch Normalization

---

# ğŸ“Œ 1. Data Preprocessing

## ğŸ¯ Goal

One major goal of preprocessing:

- Zero-centered data
- Unit-variance data

For input $x$:

$$
\tilde{x} = \frac{x - \mu}{\sigma}
$$

Where:

$$
\mu = \mathbb{E}[x], \quad
\sigma^2 = \text{Var}(x)
$$

---

## ğŸ” Why Zero-Mean?

If data is not centered:

- Gradients become biased
- Optimization zig-zags
- Slower convergence

Zero-centered data improves gradient symmetry.

---

## â“ What About Intermediate Layers?

Preprocessing works for input layer.

But what about hidden activations?

ğŸ‘‰ That leads to **Batch Normalization**

---

# ğŸš€ 2. Batch Normalization

---

# ğŸ“Œ 2.1 Basic Idea

We want:

> Zero-mean, unit-variance activations  
> At every layer

Given activations $x^{(k)}$ in a mini-batch:

$$
\hat{x}^{(k)} =
\frac{x^{(k)} - \mathbb{E}[x^{(k)}]}
{\sqrt{\text{Var}[x^{(k)}]}}
$$

---

# ğŸ“Œ 2.2 Estimating Mean and Variance

For mini-batch of size $N$:

### Mean

$$
\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{i,j}
$$

### Variance

$$
\sigma_j^2 =
\frac{1}{N} \sum_{i=1}^{N} (x_{i,j} - \mu_j)^2
$$

### Normalize

$$
\hat{x}_{i,j} =
\frac{x_{i,j} - \mu_j}
{\sqrt{\sigma_j^2 + \epsilon}}
$$

Where:

- $j$ = feature dimension
- $\epsilon$ = small constant for numerical stability

---

# ğŸ“Œ 2.3 Learnable Scale and Shift

Pure normalization may reduce model flexibility.

So we add:

$$
y^{(k)} = \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}
$$

Where:

- $\gamma$ = learnable scale
- $\beta$ = learnable shift

This allows identity mapping if needed.

---

# ğŸ“Œ 2.4 During Inference

Problem:

- At training â†’ use mini-batch statistics
- At test â†’ no batch available

Solution:

Maintain running averages:

$$
\mu_{\text{running}}, \quad
\sigma^2_{\text{running}}
$$

Use them during inference.

---

# âœ… Why BatchNorm Works

âœ” Easier training  
âœ” Better gradient flow  
âœ” Allows higher learning rate  
âœ” Faster convergence  
âœ” More robust to initialization  
âœ” Acts as mild regularizer  

---

# âš  Why NOT BatchNorm?

Issues:

- Depends on mini-batch statistics
- Problem if batch size is small
- Distribution shift between train/test
- Not ideal for non-i.i.d data

---

# ğŸ” Solution: Batch Renormalization

Paper:  
https://arxiv.org/pdf/1702.03275.pdf

Reduces train/test mismatch.

---

# ğŸ§  3. Other Normalization Methods

---

## ğŸ“¦ Layer Normalization

Paper:  
https://arxiv.org/abs/1607.06450

- Normalize across features per sample
- Works well for NLP / Transformers

---

## ğŸ–¼ Instance Normalization

Paper:  
https://arxiv.org/abs/1607.08022

- Normalize per sample per channel
- Popular in style transfer

---

## ğŸ‘¥ Group Normalization

Paper:  
https://arxiv.org/abs/1803.08494

- Split channels into groups
- Normalize within each group
- Works better for small batch sizes

---

# ğŸ Summary Table

| Method | Depends on Batch? | Works with Small Batch? | Typical Use |
|---------|------------------|--------------------------|-------------|
| BatchNorm | Yes | âŒ Not ideal | CNN |
| LayerNorm | No | âœ… Yes | Transformers |
| InstanceNorm | No | âœ… Yes | Style transfer |
| GroupNorm | No | âœ… Yes | Small-batch CNN |

---

# ğŸ¯ Final Takeaways

Deep networks benefit from:

- Proper preprocessing
- Stable activation distributions
- Controlled variance propagation

Normalization helps:

$$
\text{Input variance} \approx \text{Output variance}
$$
