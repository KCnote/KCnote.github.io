---
layout: post
title: "LSTM and GRU"
date: 2026-02-12 00:00:00 +0900
author: kang
categories: [Artificial Intelligence, Model]
tags: [Artificial Intelligenc, Model, RNN, LSTM, GRU]
pin: false
math: true
mermaid: true
---
# ðŸš€ Recurrent Neural Networks Deep Dive

## From Gradient Problems â†’ LSTM â†’ GRU â†’ Practical Guidance

------------------------------------------------------------------------

# ðŸ§  1. Why Vanilla RNN Is Not Enough

Vanilla RNN computes hidden states as:

$$
h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
$$

The key issue:

-   The same matrix $W_{hh}$ is multiplied repeatedly across time.
-   Backpropagation multiplies Jacobians repeatedly.
-   This leads to **exponential growth or decay** of gradients.

------------------------------------------------------------------------

# ðŸ”¥ 2. Exploding & Vanishing Gradients

Backpropagation through time produces:

$$
\frac{\partial L}{\partial h_k}
=
\frac{\partial L}{\partial h_t}
\prod_{i=k+1}^{t}
\frac{\partial h_i}{\partial h_{i-1}}
$$

And

$$
\frac{\partial h_t}{\partial h_{t-1}}
=
\tanh'(W_{hh} h_{t-1} + W_{xh} x_t) W_{hh}
$$

### ðŸŽ¯ Meaning

-   If largest singular value of $W_{hh}$ \> 1 â†’ exploding gradient
-   If \< 1 â†’ vanishing gradient
-   $\tanh'$ â‰¤ 1 almost always â†’ shrinking effect

Thus:

> Long sequences behave like **very deep networks**.

------------------------------------------------------------------------

# âœ‚ï¸ 3. Gradient Clipping (Exploding Fix)

If gradient norm exceeds threshold $\tau$:

$$
g \leftarrow \tau \frac{g}{\|g\|}
$$

### ðŸ’¡ Meaning

-   Preserves direction
-   Controls magnitude
-   Stabilizes training

But... this does **not fix vanishing gradients**.

------------------------------------------------------------------------

# ðŸ— 4. LSTM: Long Short-Term Memory

### ðŸ”‘ Core Idea

Create a **cell state highway** allowing gradient to flow with minimal
decay.

Instead of only hidden state $h_t$, we introduce:

-   Cell state $c_t$
-   Gating mechanisms

------------------------------------------------------------------------

## ðŸ§® LSTM Equations

### Forget Gate

$$
f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
$$

### Input Gate

$$
i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
$$

### Candidate Memory

$$
\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)
$$

### Cell Update

$$
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
$$

### Output Gate

$$
o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
$$

### Hidden State

$$
h_t = o_t \odot \tanh(c_t)
$$

------------------------------------------------------------------------

## ðŸ§  What It Really Means

  Component     Role
  ------------- ----------------------------------
  Forget gate   Decides what old memory to erase
  Input gate    Decides what new info to store
  Output gate   Controls exposure of memory
  Cell state    Gradient highway

If:

-   $f_t \approx 1$
-   $i_t \approx 0$

Then memory is preserved almost perfectly.

------------------------------------------------------------------------

# âš™ï¸ 5. GRU: Gated Recurrent Unit

Simpler alternative to LSTM.

No separate cell state.

------------------------------------------------------------------------

## ðŸ§® GRU Equations

### Reset Gate

$$
r_t = \sigma(W_r x_t + U_r h_{t-1} + b_r)
$$

### Update Gate

$$
z_t = \sigma(W_z x_t + U_z h_{t-1} + b_z)
$$

### Candidate Hidden

$$
\tilde{h}_t =
\tanh(W_h x_t + U_h (r_t \odot h_{t-1}) + b_h)
$$

### Final Hidden

$$
h_t =
(1 - z_t) \odot h_{t-1}
+
z_t \odot \tilde{h}_t
$$

------------------------------------------------------------------------

## ðŸ§  Interpretation

GRU blends:

-   Old hidden state
-   New candidate state

via a **convex combination**.

Fewer parameters than LSTM â†’ faster training.

------------------------------------------------------------------------

# ðŸ“Š 6. LSTM vs GRU Comparison

  Feature          LSTM     GRU
  ---------------- -------- ------------------
  Cell state       Yes      No
  Gates            3        2
  Parameters       More     Fewer
  Expressiveness   Higher   Slightly simpler
  Speed            Slower   Faster

------------------------------------------------------------------------

# ðŸ§­ 7. Practical Guidance

### âœ… Use LSTM when:

-   Long-range dependency is critical
-   Accuracy is priority

### âœ… Use GRU when:

-   Need faster training
-   Model size matters
-   Dataset is smaller

### ðŸš€ For Modern NLP:

> Transformers outperform RNN variants in most large-scale tasks.

------------------------------------------------------------------------

# ðŸŽ¯ Final Takeaway

Vanilla RNN â†’ unstable gradient dynamics

LSTM â†’ controlled memory highway

GRU â†’ efficient gated blending

Transformers â†’ attention-based global modeling

------------------------------------------------------------------------

# ðŸ“Œ Summary Diagram (Conceptual)

Deep sequence â‰ˆ Deep network

Repeated multiplication â†’

-   Explosion if eigenvalue \> 1
-   Vanishing if eigenvalue \< 1

LSTM introduces additive path:

$$
c_t = f_t c_{t-1} + i_t \tilde{c}_t
$$

Addition prevents exponential collapse.

------------------------------------------------------------------------

ðŸ§  **Understanding gradients = understanding deep learning stability.**
