---
layout: post
title: "Decision Trees"
date: 2026-02-09 00:00:00 +0900
author: kang
categories: [Machince Learning, Model]
tags: [Machince Learning, Mathematics, Model, Classification Trees]
pin: false
math: true
mermaid: true
---

# ğŸŒ³ Classification Trees

---

## ğŸ“˜ 1. What is a Classification Tree?

A **classification tree** is very similar to a regression tree, except:

- Regression â†’ predicts **continuous value**
- Classification â†’ predicts **discrete (categorical) class** ğŸ¯

For a classification tree, prediction is:

$$
\hat{y} = \text{most frequent class in region } R_j
$$

---

## âš™ï¸ 2. Decision Tree Algorithm (Classification)

### Step 1 â€” Partition the Feature Space ğŸ”ª

We divide the predictor space into **J disjoint regions**:

$$
R_1, R_2, ..., R_J
$$

Using **recursive binary splitting (topâ€‘down greedy)**.

For feature $x_j$ and split point $s$:

$$
R_1(j,s) = \{x \mid x_j < s\}
$$
$$
R_2(j,s) = \{x \mid x_j \ge s\}
$$

We choose $(j,s)$ minimizing classification error via impurity:

$$
\frac{n_1}{n} \, \text{Impurity}(R_1) + \frac{n_2}{n} \, \text{Impurity}(R_2)
$$

Goal ğŸ‘‰ make regions **as pure as possible** ğŸ§¼

---

## ğŸ“Š 3. Impurity Measures

### Entropy (Crossâ€‘Entropy) ğŸ”¥

$$
\text{Impurity}(R_j) = -\sum_{k=1}^{K} p_{j,k} \log p_{j,k}
$$

Where:

- $p_{j,k}$ = proportion of class $k$ in region $R_j$

#### Properties

- Pure region â†’ Entropy = **0**
- Uniform distribution â†’ Entropy = **max = log(K)**

Examples:

$$
[1,0,0,0] \Rightarrow 0
$$

$$
[0.5,0.5] \Rightarrow \log 2
$$

$$
[0.2,0.2,0.2,0.2,0.2] \Rightarrow \log 5
$$

---

## ğŸ¯ 4. Prediction in Each Region

### Majority Vote

$$
\hat{y}_j = \arg\max_k \; p_{j,k}
$$

Equivalent form:

$$
\hat{y}_j = \arg\max_k \frac{1}{n_j} \sum_{x_i \in R_j} I(y_i = k)
$$

Where:

- $n_j$ = number of samples in region
- $I(\cdot)$ = indicator function

---

## ğŸ§  5. Decision Boundary Behavior

| Model | Boundary Shape |
|------|---------------|
| Linear Model | Straight line ğŸ“ |
| Decision Tree | Axisâ€‘aligned blocks ğŸ§± |

Trees create **stepâ€‘wise rectangular decision boundaries**.

---

## ğŸ‘ 6. Pros of Decision Trees

- Very **interpretable** ğŸ‘€  
- Graphical structure easy to understand  
- Mimics **human decision making** ğŸ§   
- Handles **categorical features naturally**  

---

## ğŸ‘ 7. Cons of Decision Trees

- Often **lower predictive accuracy** than advanced models  
- High **variance / unstable** âš ï¸  
- Small data change â†’ very different tree  

---

## ğŸ§© 8. Summary

- Classification tree partitions space into regions
- Uses impurity (entropy / Gini) to choose splits
- Predicts by **majority vote**
- Produces **rectangular decision boundaries**
- Easy to interpret but can overfit
